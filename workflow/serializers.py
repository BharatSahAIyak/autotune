from jsonschema import Validator
from rest_framework import serializers

from .models import (
    Dataset,
    DatasetData,
    Examples,
    MLModel,
    Prompt,
    User,
    WorkflowConfig,
    Workflows,
)


class UserSerializer(serializers.ModelSerializer):
    class Meta:
        model = User
        fields = ("user_id", "user_name")


class MLModelSerializer(serializers.ModelSerializer):
    class Meta:
        model = MLModel
        fields = ("id", "name", "is_locally_cached")


class DatasetSerializer(serializers.ModelSerializer):
    class Meta:
        model = Dataset
        fields = ("id", "name", "is_locally_cached")


class PromptSerializer(serializers.ModelSerializer):
    class Meta:
        model = Prompt
        fields = (
            "id",
            "created_at",
            "updated_at",
            "user_prompt",
            "system_prompt",
            "source",
            "workflow",
        )
        extra_kwargs = {
            "id": {
                "read_only": True,
                "help_text": "The unique identifier of the prompt.",
            },
            "created_at": {
                "read_only": True,
                "help_text": "Timestamp when the prompt was created.",
            },
            "updated_at": {
                "read_only": True,
                "help_text": "Timestamp when the prompt was last updated.",
            },
            "user_prompt": {"help_text": "The prompt text provided by the user."},
            "system_prompt": {
                "help_text": "The prompt text generated by the system, if any."
            },
            "source": {"help_text": "The source from which the prompt was derived."},
            "workflow": {
                "read_only": False,
                "help_text": "The ID of the workflow to which the prompt is associated.",
            },
        }


class ExampleSerializer(serializers.ModelSerializer):
    class Meta:
        model = Examples
        fields = ("example_id", "prompt", "text", "label", "reason")
        extra_kwargs = {
            "example_id": {
                "help_text": "The unique identifier of the example, generated automatically.",
                "read_only": True,
            },
            "prompt": {
                "help_text": "The associated prompt data related to this example.",
                "read_only": True,
            },
            "text": {
                "help_text": "The main content of the example, stored as JSON.",
                "required": True,
            },
            "label": {
                "help_text": "The classification label assigned to the example.",
                "required": True,
            },
            "reason": {
                "help_text": "The reason or explanation for the assigned label.",
                "required": True,
            },
        }


class WorkflowDetailSerializer(serializers.ModelSerializer):
    user = UserSerializer(read_only=True)
    model = MLModelSerializer(read_only=True)
    dataset = DatasetSerializer(read_only=True)
    latest_prompt = PromptSerializer(read_only=True)
    prompts = PromptSerializer(many=True, read_only=True)
    examples = ExampleSerializer(many=True, read_only=True)
    workflow_config = serializers.PrimaryKeyRelatedField(
        queryset=WorkflowConfig.objects.all(),
        help_text="The configuration for the workflow",
    )

    class Meta:
        model = Workflows
        fields = (
            "workflow_id",
            "workflow_name",
            "total_examples",
            "split",
            "llm_model",
            "cost",
            "estimated_dataset_cost",
            "tags",
            "user",
            "dataset",
            "model",
            "examples",
            "latest_prompt",
            "prompts",
            "workflow_config",
        )
        extra_kwargs = {
            "workflow_id": {"help_text": "Unique ID for the workflow"},
            "workflow_name": {"help_text": "Name of the workflow"},
            "total_examples": {"help_text": "Total number of examples"},
            "split": {"help_text": "Data split information"},
            "llm_model": {"help_text": "Language model used"},
            "cost": {"help_text": "Cost associated with the workflow"},
            "estimated_dataset_cost": {"help_text": "Estimated dataset cost"},
            "tags": {"help_text": "Tags associated with the workflow"},
            "user": {"help_text": "User who created the workflow"},
            "dataset": {"help_text": "Associated dataset"},
            "model": {"help_text": "Associated ML model"},
            "examples": {"help_text": "Examples associated with the workflow"},
            "latest_prompt": {"help_text": "Details of the latest prompt"},
            "prompts": {"help_text": "List of all associated prompts"},
            "workflow_config": {"help_text": "Configuration for the workflow"},
        }


class WorkflowSerializer(serializers.ModelSerializer):
    examples = ExampleSerializer(many=True, required=False)
    workflow_config = serializers.PrimaryKeyRelatedField(
        queryset=WorkflowConfig.objects.all(),
        help_text="The configuration for the workflow",
    )

    class Meta:
        model = Workflows
        fields = (
            "workflow_id",
            "workflow_name",
            "total_examples",
            "split",
            "llm_model",
            "tags",
            "user",
            "examples",
            "latest_prompt",
            "workflow_config",
        )
        extra_kwargs = {
            "workflow_id": {"help_text": "Unique ID for the workflow"},
            "workflow_name": {"help_text": "Name of the workflow"},
            "total_examples": {"help_text": "Total number of examples", "min_value": 1},
            "split": {"help_text": "Data split information"},
            "llm_model": {"help_text": "Language model used"},
            "tags": {"help_text": "Tags associated with the workflow"},
            "user": {"help_text": "ID of the user who created the workflow"},
            "examples": {"help_text": "Examples associated with the workflow"},
            "latest_prompt": {"help_text": "Latest prompt for the workflow"},
            "workflow_config": {"help_text": "Configuration for the workflow"},
        }

    def validate_total_examples(self, value):
        if value <= 0:
            raise serializers.ValidationError("total_examples must be greater than 0")
        return value

    def create(self, validated_data):
        examples_data = validated_data.pop("examples", [])
        workflow = Workflows.objects.create(**validated_data)

        for example_data in examples_data:
            Examples.objects.create(workflow=workflow, **example_data)

        return workflow


class WorkflowConfigSerializer(serializers.ModelSerializer):
    class Meta:
        model = WorkflowConfig
        fields = (
            "id",
            "name",
            "system_prompt",
            "user_prompt_template",
            "schema_example",
            "temperature",
            "fields",
            "model_string",
        )


class ModelDataSerializer(serializers.Serializer):
    dataset = serializers.CharField(
        max_length=255,
        required=False,
        allow_blank=True,
        help_text="Huggingface dataset to be used for training. Either this or workflow_id must be provided.",
    )
    model = serializers.CharField(
        max_length=255,
        help_text="Name of the model to be used for training. Needs to be a valid model on Huggingface.",
    )
    model_id = serializers.UUIDField(
        required=False,
        allow_null=True,
        help_text="UUID of an existing model to be used.",
    )
    epochs = serializers.FloatField(
        required=False, default=1, help_text="Number of epochs for training."
    )
    save_path = serializers.CharField(
        max_length=255, help_text="Path where the trained model will be saved."
    )
    task_type = serializers.ChoiceField(
        choices=[
            "text_classification",
            "seq2seq",
            "embedding",
            "ner",
            "whisper_finetuning",
        ],
        help_text="Type of the training task.",
    )
    quantization_type = serializers.ChoiceField(
        choices=[
            "4-bit",
            "8-bit",
            "16-bit-dynamic",
            "16-bit-static",
        ],
        required=False, allow_null=True,
    )
    test_text = serializers.CharField(
        required=False, 
        allow_blank=True, 
        help_text="Sample text for testing the quantized model."
    )
    onnx = serializers.BooleanField(
        required=False,
        default=False,
        help_text="Convert the model to ONNX format.",
    )
    version = serializers.CharField(
        max_length=50, required=False, default="main", help_text="Version of the model."
    )
    workflow_id = serializers.UUIDField(
        required=False,
        allow_null=True,
        help_text="UUID of the workflow associated with the dataset. Either this or dataset must be provided.",
    )
    args = serializers.JSONField(
        required=False,
        default={},
        allow_null=True,
        help_text="Additional arguments for the training task.",
    )

    def validate(self, data):
        # TODO: needs to be a valid dataset on huggingface
        # TODO: Add validation for model_id
        dataset = data.get("dataset")
        workflow_id = data.get("workflow_id")

        if not dataset and not workflow_id:
            raise serializers.ValidationError(
                "Either dataset or workflow_id must be provided."
            )

        if not dataset and workflow_id:
            workflow_dataset = Dataset.objects.filter(workflow_id=workflow_id).first()
            if not workflow_dataset:
                raise serializers.ValidationError(
                    "No dataset associated with the provided workflow_id."
                )
            data["dataset"] = workflow_dataset.name

        return data


class MLModelSerializer(serializers.ModelSerializer):
    class Meta:
        model = MLModel
        fields = "__all__"
        ref_name = "MLModelSerializer"
        extra_kwargs = {
            "created_at": {
                "help_text": "The date and time when the model was created."
            },
            "updated_at": {
                "help_text": "The date and time when the model was last updated."
            },
            "id": {"help_text": "The unique identifier of the model."},
            "task": {"help_text": "The task for which the model was trained."},
            "name": {"help_text": "The name of the model."},
            "huggingface_id": {
                "help_text": "The ID of the model on Huggingface, if available."
            },
            "last_trained": {
                "help_text": "The date and time when the model was uploaded.",
                "required": False,
            },
            "latest_commit_hash": {
                "help_text": "The latest commit hash of the model, if available.",
                "required": False,
            },
            "is_trained_at_autotune": {
                "help_text": "Indicates if the model was trained at Autotune.",
                "default": False,
            },
            "is_locally_cached": {
                "help_text": "Indicates if the model is locally cached.",
                "default": False,
            },
            "trained_on": {
                "help_text": "The dataset on which the model was trained.",
                "required": False,
            },
            "label_studio_component": {
                "help_text": "Configuration for the Label Studio component.",
                "required": False,
            },
            "rendering_config": {
                "help_text": "Configuration for rendering.",
                "required": False,
            },
            "label_studio_comp": {
                "help_text": "Additional Label Studio component configuration.",
                "required": False,
            },
        }


class DatasetDataSerializer(serializers.ModelSerializer):
    dataset_id = serializers.SerializerMethodField(
        help_text="The ID of the dataset associated with this data."
    )

    class Meta:
        model = DatasetData
        fields = [
            "created_at",
            "updated_at",
            "record_id",
            "file",
            "input_string",
            "output_string",
            "input_json",
            "output_json",
            "dataset_id",
        ]
        extra_kwargs = {
            "created_at": {"help_text": "The date and time when the data was created."},
            "updated_at": {
                "help_text": "The date and time when the data was last updated."
            },
            "record_id": {"help_text": "The unique identifier of the data entry."},
            "file": {
                "help_text": "The name of the file from which the data was fetched."
            },
            "input_string": {"help_text": "The input string data."},
            "output_string": {"help_text": "The output string data."},
            "input_json": {"help_text": "The input data in JSON format."},
            "output_json": {"help_text": "The output data in JSON format."},
        }

    def get_dataset_id(self, obj):
        return obj.dataset_id

    def to_representation(self, instance):
        representation = super().to_representation(instance)
        return {
            key: value for key, value in representation.items() if value is not None
        }


class AudioDatasetSerializer(serializers.Serializer):
    dataset = serializers.CharField(max_length=255, required=False, allow_blank=True)
    workflow_id = serializers.UUIDField(required=False, allow_null=True)
    save_path = serializers.CharField(max_length=255)
    transcript_available = serializers.CharField(
        max_length=255, required=False, allow_blank=True
    )
    time_duration = serializers.FloatField(required=False, default=None)

    def validate(self, data):
        dataset_url = data.get("dataset")
        workflow_id = data.get("workflow_id")
        save_path = data.get("save_path")

        if not dataset_url and not workflow_id:

            raise serializers.ValidationError(
                "Either dataset_url or workflow_id must be provided"
            )

        if not dataset_url and workflow_id:
            workflow_dataset = Dataset.objects.filter(workflow_id=workflow_id).first()

            if not workflow_dataset:
                raise serializers.ValidationError(
                    "No dataset associated with the provided workflow_id."
                )
                data["dataset"] = workflow_dataset.urlpatterns
        if not save_path:
            raise serializers.ValidationError("save_path must be provided")

        return data


class ModelDeploySerializer(serializers.Serializer):
    service_names = serializers.CharField(max_length=255, required=True)
    finetuned_model = serializers.CharField(max_length=255, required=True)
    deployment_model = serializers.CharField(max_length=255, required=True)
    gh_workflow = serializers.CharField(max_length=255, required=True)

    def validate(self, data):
        return data
